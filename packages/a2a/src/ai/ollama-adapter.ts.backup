/**
 * @file Ollama Model Adapter Implementation
 * @description Adapter for Ollama models with fallback support
 */

import axios, { AxiosError, AxiosInstance } from 'axios';
import { z } from 'zod';
import {
  AIModelAdapter,
  AIRequest,
  AIResponse,
  AIResponseSchema,
  EmbeddingRequest,
  EmbeddingResponse,
  EmbeddingResponseSchema,
  RerankingRequest,
  RerankingResponse,
  RerankingResponseSchema,
} from './adapter.js';
import { ModelConfig } from './config.js';

/**
 * Ollama API request/response schemas
 */
const OllamaGenerateRequestSchema = z.object({
  model: z.string(),
  prompt: z.string(),
  stream: z.boolean().default(false),
  options: z
    .object({
      temperature: z.number().optional(),
      num_predict: z.number().optional(),
    })
    .optional(),
});

const OllamaEmbedRequestSchema = z.object({
  model: z.string(),
  prompt: z.string(),
});

/**
 * Ollama Model Adapter
 */
export class OllamaModelAdapter implements AIModelAdapter {
  private readonly config: ModelConfig;
  private readonly client: AxiosInstance;
  private readonly stats = {
    totalRequests: 0,
    successfulRequests: 0,
    totalResponseTime: 0,
    lastUsed: null as Date | null,
    isAvailable: true,
  };

  constructor(config: ModelConfig) {
    this.config = config;
    this.client = axios.create({
      baseURL: config.endpoint,
      timeout: config.timeout,
      headers: {
        'Content-Type': 'application/json',
      },
    });
  }

  getName(): string {
    return `Ollama-${this.config.model}`;
  }

  getVersion(): string {
    return '1.0.0';
  }

  async isHealthy(): Promise<boolean> {
    try {
      const response = await this.client.get('/api/tags', { timeout: 5000 });
      this.stats.isAvailable = response.status === 200;

      // Check if our specific model is available
      const models = response.data.models || [];
      const modelExists = models.some(
        (model: any) => model.name === this.config.model || model.name.startsWith(this.config.model),
      );

      this.stats.isAvailable = this.stats.isAvailable && modelExists;
      return this.stats.isAvailable;
    } catch (error) {
      console.warn(`Ollama health check failed: ${this.getErrorMessage(error)}`);
      this.stats.isAvailable = false;
      return false;
    }
  }

  /**
 * Ollama Adapter state
 */
export interface OllamaAdapterState {
  config: ModelConfig;
  client: AxiosInstance;
  stats: {
    totalRequests: number;
    successfulRequests: number;
    totalResponseTime: number;
    lastUsed: Date | null;
    isAvailable: boolean;
  };
}

/**
 * Create Ollama adapter
 */
export const createOllamaAdapter = (config: ModelConfig): AIModelAdapter => {
  const state: OllamaAdapterState = {
    config,
    client: axios.create({
      baseURL: config.endpoint,
      timeout: config.timeout,
      headers: {
        'Content-Type': 'application/json',
      },
    }),
    stats: {
      totalRequests: 0,
      successfulRequests: 0,
      totalResponseTime: 0,
      lastUsed: null,
      isAvailable: true,
    },
  };

  const getName = (): string => `Ollama-${state.config.model}`;

  const getVersion = (): string => '1.0.0';

  const isHealthy = async (): Promise<boolean> => {
    try {
      const response = await state.client.get('/api/tags', { timeout: 5000 });
      state.stats.isAvailable = response.status === 200;

      // Check if our specific model is available
      const models = response.data.models || [];
      const modelExists = models.some(
        (model: any) => model.name === state.config.model || model.name.startsWith(state.config.model),
      );

      state.stats.isAvailable = state.stats.isAvailable && modelExists;
      return state.stats.isAvailable;
    } catch (error) {
      console.warn(`Ollama health check failed: ${getErrorMessage(error)}`);
      state.stats.isAvailable = false;
      return false;
    }
  };

  const generateText = async (request: AIRequest): Promise<AIResponse> => {
    const startTime = Date.now();
    state.stats.totalRequests++;
    state.stats.lastUsed = new Date();

    try {
      const fullPrompt = buildPrompt(request.capability, request.prompt);

      const ollamaRequest = OllamaGenerateRequestSchema.parse({
        model: state.config.model,
        prompt: fullPrompt,
        options: {
          temperature: request.temperature || state.config.temperature,
          num_predict: request.maxTokens || state.config.maxTokens,
        },
      });

      const response = await state.client.post('/api/generate', ollamaRequest);
      const processingTime = Date.now() - startTime;

      state.stats.successfulRequests++;
      state.stats.totalResponseTime += processingTime;

      const content = response.data.response || '';
      const confidence = calculateConfidence(response.data);

      return AIResponseSchema.parse({
        content,
        confidence,
        metadata: {
          usage: response.data,
          model: response.data.model,
        },
        modelUsed: getName(),
        processingTime,
        success: true,
      });
    } catch (error) {
      const processingTime = Date.now() - startTime;
      state.stats.totalResponseTime += processingTime;

      throw new Error(`Ollama generation failed: ${getErrorMessage(error)}`);
    }
  };

  const generateEmbedding = async (request: EmbeddingRequest): Promise<EmbeddingResponse> => {
    const startTime = Date.now();
    state.stats.totalRequests++;
    state.stats.lastUsed = new Date();

    try {
      const ollamaRequest = OllamaEmbedRequestSchema.parse({
        model: state.config.model,
        prompt: request.text,
      });

      const response = await state.client.post('/api/embeddings', ollamaRequest);
      const processingTime = Date.now() - startTime;

      state.stats.successfulRequests++;
      state.stats.totalResponseTime += processingTime;

      const embedding = response.data.embedding || [];
      const dimensions = embedding.length;

      return EmbeddingResponseSchema.parse({
        embedding,
        dimensions,
        modelUsed: getName(),
        processingTime,
        success: true,
      });
    } catch (error) {
      const processingTime = Date.now() - startTime;
      state.stats.totalResponseTime += processingTime;

      throw new Error(`Ollama embedding failed: ${getErrorMessage(error)}`);
    }
  };

  const rerank = async (request: RerankingRequest): Promise<RerankingResponse> => {
    const startTime = Date.now();
    state.stats.totalRequests++;
    state.stats.lastUsed = new Date();

    try {
      // For reranking, we'll use the chat API with a specialized prompt
      const rerankPrompt = buildRerankPrompt(request.query, request.items);

      const chatRequest: AIRequest = {
        prompt: rerankPrompt,
        capability: 'priority_ranking' as any,
        maxTokens: 2048,
        temperature: 0.1,
      };

      const response = await generateText(chatRequest);
      const processingTime = Date.now() - startTime;

      // Parse the reranking response
      const rankedItems = parseRerankingResponse(response.content, request.items);

      return RerankingResponseSchema.parse({
        rankedItems: rankedItems.slice(0, request.topK || 10),
        modelUsed: getName(),
        processingTime,
        success: true,
      });
    } catch (error) {
      const processingTime = Date.now() - startTime;
      state.stats.totalResponseTime += processingTime;

      throw new Error(`Ollama reranking failed: ${getErrorMessage(error)}`);
    }
  };

  const getStats = () => ({
    totalRequests: state.stats.totalRequests,
    successfulRequests: state.stats.successfulRequests,
    averageResponseTime: state.stats.totalRequests > 0
      ? state.stats.totalResponseTime / state.stats.totalRequests
      : 0,
    lastUsed: state.stats.lastUsed,
    isAvailable: state.stats.isAvailable,
  });

  const cleanup = async (): Promise<void> => {
    // Ollama doesn't require special cleanup
  };

  const buildPrompt = (capability: string, userPrompt: string): string => {
    switch (capability) {
      case 'semantic_routing':
        return `You are an expert at analyzing message content and determining optimal routing strategies for agent-to-agent communication. Respond with clear, actionable routing decisions.

${userPrompt}`;

      case 'message_validation':
        return `You are a security expert specializing in message validation and anomaly detection. Analyze the content for potential issues and respond with validation results.

${userPrompt}`;

      case 'load_balancing':
        return `You are a load balancing specialist. Analyze message complexity and agent workloads to make optimal distribution decisions.

${userPrompt}`;

      case 'priority_ranking':
        return `You are an expert at ranking and prioritizing items based on relevance, urgency, and business value. Provide clear ranking rationale.

${userPrompt}`;

      default:
        return `You are a helpful AI assistant specialized in agent-to-agent communication processing.

${userPrompt}`;
    }
  };

  const calculateConfidence = (responseData: any): number => {
    // Use response length and other metrics if available
    const response = responseData.response || '';
    if (response.length > 10) {
      // Simple heuristic: longer responses tend to be more confident
      const ratio = Math.min(response.length / 100, 1);
      return Math.max(0.1, ratio);
    }
    return 0.8; // Default confidence
  };

  const buildRerankPrompt = (query: string, items: string[]): string => {
    const itemsList = items.map((item, index) => `${index}: ${item}`).join('\n');

    return `Rank the following items by relevance to the query

  async generateEmbedding(request: EmbeddingRequest): Promise<EmbeddingResponse> {
    const startTime = Date.now();
    this.stats.totalRequests++;
    this.stats.lastUsed = new Date();

    try {
      const ollamaRequest = OllamaEmbedRequestSchema.parse({
        model: this.config.model,
        prompt: request.text,
      });

      const response = await this.client.post('/api/embeddings', ollamaRequest);
      const processingTime = Date.now() - startTime;

      this.stats.successfulRequests++;
      this.stats.totalResponseTime += processingTime;

      const embedding = response.data.embedding || [];
      const dimensions = embedding.length;

      return EmbeddingResponseSchema.parse({
        embedding,
        dimensions,
        modelUsed: this.getName(),
        processingTime,
        success: true,
      });
    } catch (error) {
      const processingTime = Date.now() - startTime;
      this.stats.totalResponseTime += processingTime;

      throw new Error(`Ollama embedding failed: ${this.getErrorMessage(error)}`);
    }
  }

  async rerank(request: RerankingRequest): Promise<RerankingResponse> {
    const startTime = Date.now();
    this.stats.totalRequests++;
    this.stats.lastUsed = new Date();

    try {
      // For reranking, we'll use the generate API with a specialized prompt
      const rerankPrompt = this.buildRerankPrompt(request.query, request.items);

      const chatRequest: AIRequest = {
        prompt: rerankPrompt,
        capability: 'priority_ranking' as any,
        maxTokens: 2048,
        temperature: 0.1,
      };

      const response = await this.generateText(chatRequest);
      const processingTime = Date.now() - startTime;

      // Parse the reranking response
      const rankedItems = this.parseRerankingResponse(response.content, request.items);

      return RerankingResponseSchema.parse({
        rankedItems: rankedItems.slice(0, request.topK || 10),
        modelUsed: this.getName(),
        processingTime,
        success: true,
      });
    } catch (error) {
      const processingTime = Date.now() - startTime;
      this.stats.totalResponseTime += processingTime;

      throw new Error(`Ollama reranking failed: ${this.getErrorMessage(error)}`);
    }
  }

  getStats() {
    return {
      totalRequests: this.stats.totalRequests,
      successfulRequests: this.stats.successfulRequests,
      averageResponseTime:
        this.stats.totalRequests > 0 ? this.stats.totalResponseTime / this.stats.totalRequests : 0,
      lastUsed: this.stats.lastUsed,
      isAvailable: this.stats.isAvailable,
    };
  }

  async cleanup(): Promise<void> {
    // Ollama doesn't require special cleanup
  }

  /**
   * Build full prompt with system context
   */
  private buildPrompt(capability: string, userPrompt: string): string {
    const systemPrompt = this.getSystemPrompt(capability);
    return `${systemPrompt}\n\nUser: ${userPrompt}\n\nAssistant:`;
  }

  /**
   * Get system prompt based on capability
   */
  private getSystemPrompt(capability: string): string {
    switch (capability) {
      case 'semantic_routing':
        return 'You are an expert at analyzing message content and determining optimal routing strategies for agent-to-agent communication. Respond with clear, actionable routing decisions.';

      case 'message_validation':
        return 'You are a security expert specializing in message validation and anomaly detection. Analyze the content for potential issues and respond with validation results.';

      case 'load_balancing':
        return 'You are a load balancing specialist. Analyze message complexity and agent workloads to make optimal distribution decisions.';

      case 'priority_ranking':
        return 'You are an expert at ranking and prioritizing items based on relevance, urgency, and business value. Provide clear ranking rationale.';

      default:
        return 'You are a helpful AI assistant specialized in agent-to-agent communication processing.';
    }
  }

  /**
   * Calculate confidence score from response
   */
  private calculateConfidence(responseData: any): number {
    // Ollama doesn't provide logprobs, so use heuristics
    const responseLength = responseData.response?.length || 0;
    const evalCount = responseData.eval_count || 0;

    if (evalCount > 0 && responseLength > 0) {
      // Longer, more thoughtful responses tend to be more confident
      const lengthFactor = Math.min(1.0, responseLength / 500);
      const evalFactor = Math.min(1.0, evalCount / 100);
      return ((lengthFactor + evalFactor) / 2) * 0.9; // Cap at 0.9
    }

    return 0.7; // Default confidence for Ollama
  }

  /**
   * Build reranking prompt
   */
  private buildRerankPrompt(query: string, items: string[]): string {
    const itemsList = items.map((item, index) => `${index}: ${item}`).join('\n');

    return `Rank the following items by relevance to the query "${query}". 
Return only the indices in order of relevance (most relevant first), separated by commas.

Items:
${itemsList}

Ranking (indices only):`;
  }

  /**
   * Parse reranking response
   */
  private parseRerankingResponse(response: string, originalItems: string[]) {
    try {
      // Extract indices from response
      const indices = response
        .trim()
        .split(/[,\s]+/)
        .map((idx) => parseInt(idx.trim()))
        .filter((idx) => !isNaN(idx) && idx >= 0 && idx < originalItems.length);

      return indices.map((index, rank) => ({
        index,
        score: 1.0 - rank / indices.length, // Higher score for higher rank
        content: originalItems[index],
      }));
    } catch (error) {
      console.warn(`Failed to parse Ollama reranking response: ${this.getErrorMessage(error)}`);
      // Fallback: return original order
      return originalItems.map((content, index) => ({
        index,
        score: 1.0 - index / originalItems.length,
        content,
      }));
    }
  }

  /**
   * Extract error message from axios error
   */
  private getErrorMessage(error: unknown): string {
    if (error instanceof AxiosError) {
      return error.response?.data?.error || error.message;
    }
    return error instanceof Error ? error.message : 'Unknown error';
  }
}
