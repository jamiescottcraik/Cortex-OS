# OrbStack-specific service additions for Cortex-OS
version: '3.8'

services:
  # Add Rust-based services
  cortex-code-cli:
    container_name: cortexos_code_cli
    build:
      context: ../../
      dockerfile: apps/cortex-code/Dockerfile
      platforms:
        - linux/arm64
        - linux/amd64
    profiles: ["dev-full", "tools"]
    environment:
      RUST_LOG: debug
      CODEX_CACHE_DIR: /app/cache
    volumes:
      - ../../:/workspace:ro
      - codex_cache:/app/cache
    working_dir: /workspace
    mem_limit: "512m"
    labels:
      orbstack.optimize: "true"
      orbstack.service: "cortex-code-cli"
      orbstack.tier: "tools"
      orbstack.rosetta: "false"
    command: ["codex", "--help"]

  cortex-code-mcp:
    container_name: cortexos_code_mcp
    build:
      context: ../../
      dockerfile: apps/cortex-code/Dockerfile
      platforms:
        - linux/arm64
        - linux/amd64
    profiles: ["mcp", "dev-full"]
    environment:
      RUST_LOG: debug
      MCP_SERVER_PORT: 8004
    ports:
      - "8004:8004"
    volumes:
      - codex_cache:/app/cache
    mem_limit: "256m"
    labels:
      orbstack.optimize: "true"
      orbstack.service: "cortex-code-mcp"
      orbstack.tier: "integration"
      orbstack.rosetta: "false"
    command: ["codex-mcp-server", "--port", "8004"]

  cortex-code-tui:
    container_name: cortexos_code_tui
    build:
      context: ../../
      dockerfile: apps/cortex-code/Dockerfile
      platforms:
        - linux/arm64
        - linux/amd64
    profiles: ["tui"]
    environment:
      RUST_LOG: debug
      TERM: xterm-256color
    volumes:
      - ../../:/workspace:ro
    working_dir: /workspace
    stdin_open: true
    tty: true
    mem_limit: "256m"
    labels:
      orbstack.optimize: "true"
      orbstack.service: "cortex-code-tui"
      orbstack.tier: "ui"
      orbstack.rosetta: "false"
    command: ["codex-tui"]

  # Python ML services
  cortex-py-ml:
    container_name: cortexos_py_ml
    build:
      context: ../../
      dockerfile: apps/cortex-py/Dockerfile
      platforms:
        - linux/arm64
        - linux/amd64
    profiles: ["ml", "dev-full"]
    environment:
      PYTHONPATH: /app/src
      LOG_LEVEL: DEBUG
      TRANSFORMERS_CACHE: /app/cache/transformers
      HF_HOME: /app/cache/huggingface
    ports:
      - "8005:8000"
    volumes:
      - python_cache:/app/cache
      - model_cache:/app/models
      - huggingface_cache:/app/cache/huggingface
    mem_limit: "2g"
    labels:
      orbstack.optimize: "true"
      orbstack.service: "python-ml"
      orbstack.tier: "ai"
      orbstack.python: "true"
    command: ["python", "-m", "uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]

volumes:
  codex_cache:
    driver: local
    labels:
      orbstack.volume: "cache"
      orbstack.service: "codex"
  python_cache:
    driver: local
    labels:
      orbstack.volume: "cache"
      orbstack.service: "python"
  model_cache:
    driver: local
    labels:
      orbstack.volume: "models"
      orbstack.service: "ai"
  huggingface_cache:
    driver: local
    labels:
      orbstack.volume: "cache"
      orbstack.service: "huggingface"