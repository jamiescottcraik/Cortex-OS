#!/usr/bin/env python3
"""
Demo: Complete MLX Integration in Cortex-OS
Shows all available models and capabilities
"""

import os

# Configure cache
os.environ.setdefault("HF_HOME", "/Volumes/ExternalSSD/huggingface_cache")


def show_mlx_models():
    """Display all available MLX models in the system"""

    print("üöÄ Cortex-OS MLX Integration Demo")
    print("=" * 50)

    models = {
        "üí¨ Chat Models": {
            "qwen3-coder-30b-mlx": "30B parameters ‚Ä¢ Coding expert ‚Ä¢ 32k context",
            "qwen2.5-vl-3b-mlx": "3B parameters ‚Ä¢ Vision + Language ‚Ä¢ 32k context",
            "qwen2.5-0.5b-mlx": "0.5B parameters ‚Ä¢ Lightweight chat ‚Ä¢ 32k context",
            "mixtral-8x7b-mlx": "8x7B MoE ‚Ä¢ Advanced reasoning ‚Ä¢ 32k context",
            "gemma2-2b-mlx": "2B parameters ‚Ä¢ Google model ‚Ä¢ 8k context",
            "glm-4.5-mlx": "12B parameters ‚Ä¢ Multilingual ‚Ä¢ 32k context",
            "phi3-mini-mlx": "2B parameters ‚Ä¢ Microsoft model ‚Ä¢ 4k context",
            "gpt-oss-20b-mlx": "20B parameters ‚Ä¢ Open source GPT ‚Ä¢ 8k context",
        },
        "üîç Embedding Models": {
            "qwen3-embedding-0.6b-mlx": "0.6B ‚Ä¢ 1536 dims ‚Ä¢ Fast inference",
            "qwen3-embedding-4b-mlx": "4B ‚Ä¢ 1536 dims ‚Ä¢ High quality",
            "qwen3-embedding-8b-mlx": "8B ‚Ä¢ 1536 dims ‚Ä¢ Best quality",
        },
        "üìä Reranking Models": {
            "qwen3-reranker-4b-mlx": "4B ‚Ä¢ Document reranking ‚Ä¢ Search optimization",
        },
    }

    for category, model_list in models.items():
        print(f"\n{category}")
        print("-" * 30)
        for name, desc in model_list.items():
            print(f"  {name:<25} ‚îÇ {desc}")

    print(f"\nüéØ Total Models Available: {sum(len(v) for v in models.values())}")
    print(f"üìÅ Cache Location: {os.environ.get('HF_HOME')}")

    print("\n‚ö° Fallback Chain:")
    print("  1. MLX (ExternalSSD) ‚Üí 2. Ollama (Local) ‚Üí 3. Frontier (Cloud)")

    print("\nüîß Usage Examples:")
    print("  # TypeScript (Model Gateway)")
    print("  const response = await modelRouter.generateChat({")
    print("    messages: [{role: 'user', content: 'Write Python code'}],")
    print("    model: 'qwen3-coder-30b-mlx'")
    print("  });")
    print()
    print("  # Python (Direct MLX)")
    print("  from mlx_lm import load, generate")
    print("  model, tokenizer = load('lmstudio-community/gpt-oss-20b-MLX-8bit')")
    print("  response = generate(model, tokenizer, prompt='Story:', max_tokens=500)")

    print("\n‚úÖ Features:")
    print("  ‚úì Timeout protection (30s)")
    print("  ‚úì Retry logic (2 attempts)")
    print("  ‚úì Memory validation")
    print("  ‚úì Performance budgets")
    print("  ‚úì Comprehensive test coverage")
    print("  ‚úì TDD compliance")

    print("\nüéâ Ready for Production!")


if __name__ == "__main__":
    show_mlx_models()
