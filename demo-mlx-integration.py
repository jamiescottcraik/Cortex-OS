#!/usr/bin/env python3
"""
Demo: Complete MLX Integration in Cortex-OS
Shows all available models and capabilities
"""

import os

# Configure cache
os.environ.setdefault('HF_HOME', '/Volumes/ExternalSSD/huggingface_cache')

def show_mlx_models():
    """Display all available MLX models in the system"""
    
    print("üöÄ Cortex-OS MLX Integration Demo")
    print("=" * 50)
    
    models = {
        "üí¨ Chat Models": {
            "qwen3-coder-30b-mlx": "30B parameters ‚Ä¢ Coding expert ‚Ä¢ 32k context",
            "qwen2.5-vl-3b-mlx": "3B parameters ‚Ä¢ Vision + Language ‚Ä¢ 32k context", 
            "qwen2.5-0.5b-mlx": "0.5B parameters ‚Ä¢ Lightweight chat ‚Ä¢ 32k context",
            "mixtral-8x7b-mlx": "8x7B MoE ‚Ä¢ Advanced reasoning ‚Ä¢ 32k context",
            "gemma2-2b-mlx": "2B parameters ‚Ä¢ Google model ‚Ä¢ 8k context",
            "glm-4.5-mlx": "12B parameters ‚Ä¢ Multilingual ‚Ä¢ 32k context",
            "phi3-mini-mlx": "2B parameters ‚Ä¢ Microsoft model ‚Ä¢ 4k context",
            "gpt-oss-20b-mlx": "20B parameters ‚Ä¢ Open source GPT ‚Ä¢ 8k context",
        },
        "üîç Embedding Models": {
            "qwen3-embedding-0.6b-mlx": "0.6B ‚Ä¢ 1536 dims ‚Ä¢ Fast inference",
            "qwen3-embedding-4b-mlx": "4B ‚Ä¢ 1536 dims ‚Ä¢ High quality",
            "qwen3-embedding-8b-mlx": "8B ‚Ä¢ 1536 dims ‚Ä¢ Best quality",
        },
        "üìä Reranking Models": {
            "qwen3-reranker-4b-mlx": "4B ‚Ä¢ Document reranking ‚Ä¢ Search optimization",
        }
    }
    
    for category, model_list in models.items():
        print(f"\n{category}")
        print("-" * 30)
        for name, desc in model_list.items():
            print(f"  {name:<25} ‚îÇ {desc}")
    
    print(f"\nüéØ Total Models Available: {sum(len(v) for v in models.values())}")
    print(f"üìÅ Cache Location: {os.environ.get('HF_HOME')}")
    
    print(f"\n‚ö° Fallback Chain:")
    print(f"  1. MLX (ExternalSSD) ‚Üí 2. Ollama (Local) ‚Üí 3. Frontier (Cloud)")
    
    print(f"\nüîß Usage Examples:")
    print(f"  # TypeScript (Model Gateway)")
    print(f"  const response = await modelRouter.generateChat({{")
    print(f"    messages: [{{role: 'user', content: 'Write Python code'}}],")
    print(f"    model: 'qwen3-coder-30b-mlx'")
    print(f"  }});")
    print()
    print(f"  # Python (Direct MLX)")
    print(f"  from mlx_lm import load, generate")
    print(f"  model, tokenizer = load('lmstudio-community/gpt-oss-20b-MLX-8bit')")
    print(f"  response = generate(model, tokenizer, prompt='Story:', max_tokens=500)")
    
    print(f"\n‚úÖ Features:")
    print(f"  ‚úì Timeout protection (30s)")
    print(f"  ‚úì Retry logic (2 attempts)")
    print(f"  ‚úì Memory validation")
    print(f"  ‚úì Performance budgets")
    print(f"  ‚úì Comprehensive test coverage")
    print(f"  ‚úì TDD compliance")
    
    print(f"\nüéâ Ready for Production!")

if __name__ == "__main__":
    show_mlx_models()